"""
é“¶è¡Œæˆä¿¡ä¸šåŠ¡è°ƒæŸ¥æŠ¥å‘Šè‡ªåŠ¨å¡«å†™åº”ç”¨
"""

import os
import tempfile
import re
import json
import base64
import hashlib
from pathlib import Path
from urllib.parse import urlparse, quote_plus
from urllib.robotparser import RobotFileParser
from io import BytesIO
import subprocess
import shutil

import chromadb
import ollama
import streamlit as st
import pandas as pd
from chromadb.config import Settings
from chromadb.utils.embedding_functions import OllamaEmbeddingFunction
from crawl4ai import AsyncWebCrawler, BrowserConfig, CacheMode, CrawlerRunConfig
from crawl4ai.content_filter_strategy import BM25ContentFilter
from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator
from crawl4ai.models import CrawlResult
from langchain_community.document_loaders import UnstructuredMarkdownLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter

# å¯¼å…¥OCRæ¨¡å—
import sys
sys.path.append('.')
from ocr import MistralOCRProcessor

# è¯­è¨€é…ç½®
TEXTS = {
    "zh": {
        "page_title": "é“¶è¡Œæˆä¿¡ä¸šåŠ¡è°ƒæŸ¥æŠ¥å‘Šè‡ªåŠ¨å¡«å†™ç³»ç»Ÿ",
        "header": "ğŸ¦ é“¶è¡Œæˆä¿¡ä¸šåŠ¡è°ƒæŸ¥æŠ¥å‘Šè‡ªåŠ¨å¡«å†™ç³»ç»Ÿ",
        "applicant_name": "ç”³è¯·äººåç§°ï¼ˆä¼ä¸š/å…¬å¸ï¼‰",
        "applicant_placeholder": "è¯·è¾“å…¥ç”³è¯·äººä¼ä¸š/å…¬å¸åç§°",
        "upload_report": "ä¸Šä¼ é“¶è¡Œæˆä¿¡ä¸šåŠ¡è°ƒæŸ¥æŠ¥å‘Šï¼ˆPDFæ ¼å¼ï¼‰",
        "process_button": "ğŸš€ å¼€å§‹å¤„ç†",
        "searching": "ğŸ” æ­£åœ¨æœç´¢ç”³è¯·äººä¿¡æ¯...",
        "found_urls": "ğŸ“ æ‰¾åˆ° {} ä¸ªç›¸å…³ç½‘é¡µ",
        "ocr_processing": "ğŸ”„ æ­£åœ¨è¿›è¡ŒOCRå¤„ç†...",
        "report_parsing": "ğŸ“‹ æ­£åœ¨è§£ææŠ¥å‘Šç»“æ„...",
        "generating_summary": "ğŸ“Š æ­£åœ¨ç”Ÿæˆè´¢æŠ¥æ€»ç»“...",
        "filling_report": "âœï¸ æ­£åœ¨å¡«å†™æŠ¥å‘Šç¬¬ {} éƒ¨åˆ†ï¼š{}",
        "download_summary": "ğŸ“¥ ä¸‹è½½è´¢æŠ¥æ€»ç»“æŠ¥å‘Š",
        "download_filled": "ğŸ“¥ ä¸‹è½½å¡«å†™å®Œæˆçš„æŠ¥å‘Š",
        "processing_complete": "âœ… å¤„ç†å®Œæˆï¼",
        "error_occurred": "âŒ å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯",
    }
}

def get_text(key: str) -> str:
    """è·å–å½“å‰è¯­è¨€çš„æ–‡æœ¬"""
    return TEXTS["zh"].get(key, key)

# ç³»ç»Ÿæç¤ºè¯
FINANCIAL_SUMMARY_PROMPT = """
ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„é“¶è¡Œè°ƒæŸ¥å‘˜ã€‚è¯·åŸºäºæä¾›çš„ç”³è¯·äººä¼ä¸šä¿¡æ¯ï¼Œç”Ÿæˆä¸€ä»½è¯¦ç»†çš„è´¢åŠ¡çŠ¶å†µæ€»ç»“æŠ¥å‘Šã€‚

è¯·åŒ…å«ä»¥ä¸‹æ–¹é¢çš„åˆ†æï¼š
1. **ä¼ä¸šåŸºæœ¬æƒ…å†µ** - å…¬å¸æ€§è´¨ã€ç»è¥èŒƒå›´ã€æ³¨å†Œèµ„æœ¬ç­‰
2. **è´¢åŠ¡çŠ¶å†µåˆ†æ** - èµ„äº§è´Ÿå€ºæƒ…å†µã€ç›ˆåˆ©èƒ½åŠ›ã€ç°é‡‘æµçŠ¶å†µ
3. **ç»è¥æƒ…å†µåˆ†æ** - ä¸»è¥ä¸šåŠ¡ã€å¸‚åœºåœ°ä½ã€ç»è¥é£é™©ç­‰
4. **ä¿¡ç”¨çŠ¶å†µè¯„ä¼°** - å†å²ä¿¡ç”¨è®°å½•ã€è¿˜æ¬¾èƒ½åŠ›åˆ†æ
5. **é£é™©å› ç´ è¯†åˆ«** - æ½œåœ¨é£é™©ç‚¹å’Œé£é™©ç­‰çº§è¯„ä¼°

è¯·åŸºäºæä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œè¯¦ç»†å¡«å†™ä»¥ä¸Šå„é¡¹å†…å®¹ã€‚å¦‚æœæŸäº›ä¿¡æ¯ä¸è¶³ï¼Œè¯·æ˜ç¡®è¯´æ˜æ•°æ®æ¥æºé™åˆ¶ã€‚

ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š{context}
"""

REPORT_FILLING_PROMPT = """
ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„é“¶è¡Œä¿¡è´·ä¸šåŠ¡ä¸“å®¶ã€‚è¯·åŸºäºæä¾›çš„ç”³è¯·äººä¼ä¸šä¿¡æ¯ï¼Œä¸ºé“¶è¡Œæˆä¿¡ä¸šåŠ¡è°ƒæŸ¥æŠ¥å‘Šçš„ä»¥ä¸‹éƒ¨åˆ†æä¾›è¯¦ç»†çš„å¡«å†™å†…å®¹ã€‚

éœ€è¦å¡«å†™çš„æŠ¥å‘Šéƒ¨åˆ†ï¼š
{section_content}

è¯·æŒ‰ç…§ä»¥ä¸‹è¦æ±‚å¡«å†™ï¼š
1. **ä¸¥æ ¼ä¿æŒåŸæœ‰çš„markdownæ ¼å¼å’Œç»“æ„**
2. **åªåœ¨éœ€è¦å¡«å†™çš„ç©ºç™½å¤„ã€è¡¨æ ¼ç©ºç™½å•å…ƒæ ¼ã€æˆ–æ˜æ˜¾éœ€è¦è¡¥å……çš„åœ°æ–¹è¿›è¡Œå¡«å†™**
3. **ä¸è¦æ–°å¢åŸæŠ¥å‘Šä¸­æ²¡æœ‰çš„ç« èŠ‚ã€è¡¨æ ¼æˆ–å†…å®¹å—**
4. **ä¸è¦åˆ é™¤æˆ–å¤§å¹…ä¿®æ”¹åŸæœ‰çš„æ ‡é¢˜ã€æ ¼å¼ã€è¡¨æ ¼ç»“æ„**
5. å†…å®¹å¿…é¡»å‡†ç¡®ã€ä¸“ä¸šã€è¯¦å®ï¼Œç¬¦åˆé“¶è¡Œæˆä¿¡ä¸šåŠ¡è§„èŒƒ
6. æ•°æ®å¼•ç”¨è¦æœ‰æ¥æºä¾æ®ï¼Œåˆ†æè¦å®¢è§‚ä¸­ç«‹

ç”³è¯·äººä¼ä¸šä¿¡æ¯ä¸Šä¸‹æ–‡ï¼š
{context}

è¯·ç›´æ¥è¾“å‡ºå¡«å†™åçš„å®Œæ•´å†…å®¹ï¼Œä¸¥æ ¼ä¿æŒåŸæœ‰çš„markdownæ ¼å¼ï¼Œåªåœ¨ç©ºç™½æˆ–éœ€è¦å¡«å†™çš„åœ°æ–¹æ·»åŠ ç›¸åº”ä¿¡æ¯ã€‚
"""

class CreditReportProcessor:
    def __init__(self):
        """åˆå§‹åŒ–å¤„ç†å™¨"""
        self.collection = None
        self.chroma_client = None
        self.ocr_processor = None
        # OCRç¼“å­˜ç›®å½•
        self.ocr_cache_dir = Path("./ocr_cache")
        self.ocr_cache_dir.mkdir(exist_ok=True)
        self.init_components()
    
    def init_components(self):
        """åˆå§‹åŒ–ç»„ä»¶"""
        # åˆå§‹åŒ–å‘é‡æ•°æ®åº“
        self.init_vector_db()
        
        # OCRå¤„ç†å™¨å°†åœ¨éœ€è¦æ—¶åŠ¨æ€åˆå§‹åŒ–
    
    def init_ocr_processor(self, api_key: str):
        """åŠ¨æ€åˆå§‹åŒ–OCRå¤„ç†å™¨"""
        if api_key:
            self.ocr_processor = MistralOCRProcessor(api_key)
        else:
            self.ocr_processor = None
    
    def get_file_hash(self, file_content: bytes) -> str:
        """è®¡ç®—æ–‡ä»¶å†…å®¹çš„SHA256å“ˆå¸Œå€¼"""
        return hashlib.sha256(file_content).hexdigest()
    
    def get_cached_ocr_result(self, file_hash: str) -> str:
        """è·å–ç¼“å­˜çš„OCRç»“æœ"""
        cache_file = self.ocr_cache_dir / f"{file_hash}.md"
        if cache_file.exists():
            with open(cache_file, 'r', encoding='utf-8') as f:
                return f.read()
        return None
    
    def save_ocr_result_to_cache(self, file_hash: str, markdown_content: str):
        """ä¿å­˜OCRç»“æœåˆ°ç¼“å­˜"""
        cache_file = self.ocr_cache_dir / f"{file_hash}.md"
        with open(cache_file, 'w', encoding='utf-8') as f:
            f.write(markdown_content)
    
    def init_vector_db(self):
        """åˆå§‹åŒ–å‘é‡æ•°æ®åº“"""
        ollama_ef = OllamaEmbeddingFunction(
            url="http://localhost:11434/api/embeddings",
            model_name="paraphrase-multilingual:latest",
        )

        self.chroma_client = chromadb.PersistentClient(
            path="./credit-report-db", 
            settings=Settings(anonymized_telemetry=False)
        )
        
        self.collection = self.chroma_client.get_or_create_collection(
            name="applicant_info",
            embedding_function=ollama_ef,
            metadata={"hnsw:space": "cosine"},
        )
    
    def search_applicant_info(self, applicant_name: str, num_results: int = 20) -> list[str]:
        """æœç´¢ç”³è¯·äººä¼ä¸šä¿¡æ¯"""
        import asyncio
        
        try:
            # è·å–è°ƒè¯•æ¨¡å¼çŠ¶æ€
            debug_mode = getattr(st.session_state, 'debug_mode', False)
            
            # ç®€åŒ–æœç´¢ç­–ç•¥ - åªä½¿ç”¨ç”³è¯·äººåç§°è¿›è¡Œæœç´¢
            search_query = applicant_name
            
            # æ ¹æ®è°ƒè¯•æ¨¡å¼æ˜¾ç¤ºä¸åŒçº§åˆ«çš„ä¿¡æ¯
            if debug_mode:
                st.info(f"ğŸ“ æœç´¢æŸ¥è¯¢: {search_query}")
            
            # ç›´æ¥æœç´¢ç”³è¯·äººåç§°
            urls = asyncio.run(self.get_web_urls(search_query, num_results, debug_mode))
            
            if debug_mode:
                st.success(f"ğŸ¯ æ‰¾åˆ° {len(urls)} ä¸ªç›¸å…³é“¾æ¥")
            else:
                st.success(f"ğŸ¯ æ‰¾åˆ° {len(urls)} ä¸ªç›¸å…³é“¾æ¥")
            
            return urls
            
        except Exception as e:
            st.error(f"æœç´¢å¤±è´¥: {str(e)}")
            return []
    
    async def get_web_urls(self, search_term: str, num_results: int = 5, debug_mode: bool = False) -> list[str]:
        """é€šè¿‡Bingæœç´¢è·å–URL"""
        try:
            query = search_term
            # ç¡®ä¿æŸ¥è¯¢å­—ç¬¦ä¸²çš„ç¼–ç æ­£ç¡®
            encoded_query = quote_plus(query, encoding='utf-8')
            search_url = f"https://www.bing.com/search?q={encoded_query}&count={num_results}"
            
            # æ ¹æ®è°ƒè¯•æ¨¡å¼æ˜¾ç¤ºä¿¡æ¯
            if debug_mode:
                st.info(f"ğŸ” æœç´¢æŸ¥è¯¢: {query}")
                st.info(f"ğŸŒ æœç´¢URLé•¿åº¦: {len(search_url)} å­—ç¬¦")
                st.code(search_url, language="text")
            
            # æ€»æ˜¯æ‰“å°åˆ°consoleç”¨äºè°ƒè¯•ï¼Œå³ä½¿ä¸åœ¨debugæ¨¡å¼
            print(f"=== æœç´¢è°ƒè¯•ä¿¡æ¯ ===")
            print(f"åŸå§‹æŸ¥è¯¢: {query}")
            print(f"ç¼–ç æŸ¥è¯¢: {encoded_query}")
            print(f"å®Œæ•´URL: {search_url}")
            print(f"URLé•¿åº¦: {len(search_url)} å­—ç¬¦")
            print("==================")
            
            crawler_config = CrawlerRunConfig(
                excluded_tags=["nav", "footer", "header", "form", "img"],
                only_text=False,
                exclude_social_media_links=False,
                keep_data_attributes=False,
                cache_mode=CacheMode.BYPASS,
                remove_overlay_elements=True,
                user_agent="Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
                page_timeout=30000,
                delay_before_return_html=3.0,
            )
            
            browser_config = BrowserConfig(headless=True, text_mode=False, light_mode=True)
            
            async with AsyncWebCrawler(config=browser_config) as crawler:
                # ç¡®ä¿ä¼ é€’çš„URLæ˜¯å®Œæ•´çš„
                print(f"ä¼ é€’ç»™crawlerçš„URL: {search_url}")
                result = await crawler.arun(url=search_url, config=crawler_config)
                
                if not result.html:
                    if debug_mode:
                        st.warning(f"æœªè·å–åˆ°æœç´¢ç»“æœé¡µé¢å†…å®¹ï¼ŒæŸ¥è¯¢: {query}")
                    print(f"æœªè·å–åˆ°HTMLå†…å®¹ï¼ŒURL: {search_url}")
                    return []
                
                urls = self.extract_bing_links(result.html, num_results)
                if debug_mode:
                    if urls:
                        st.success(f"âœ… ä»æŸ¥è¯¢ '{query}' ä¸­æ‰¾åˆ° {len(urls)} ä¸ªé“¾æ¥")
                        st.write("æ‰¾åˆ°çš„é“¾æ¥:")
                        for i, url in enumerate(urls, 1):
                            st.write(f"{i}. {url}")
                    else:
                        st.warning(f"âš ï¸ æŸ¥è¯¢ '{query}' æœªæ‰¾åˆ°æœ‰æ•ˆé“¾æ¥")
                
                print(f"æå–åˆ° {len(urls)} ä¸ªé“¾æ¥")
                return self.check_robots_txt(urls)
                
        except Exception as e:
            error_msg = f"ç½‘é¡µæœç´¢å¤±è´¥ï¼ŒæŸ¥è¯¢: {search_term}, é”™è¯¯: {str(e)}"
            if debug_mode:
                st.error(error_msg)
            else:
                # åœ¨éè°ƒè¯•æ¨¡å¼ä¸‹ï¼Œåªè®°å½•åˆ°æ—¥å¿—ï¼Œä¸æ˜¾ç¤ºç»™ç”¨æˆ·
                print(error_msg)
            return []
    
    def extract_bing_links(self, html: str, max_results: int = 5) -> list[str]:
        """ä»Bingæœç´¢ç»“æœä¸­æå–é“¾æ¥"""
        urls = []
        
        if getattr(st.session_state, 'debug_mode', False):
            print(f"HTMLå†…å®¹é•¿åº¦: {len(html)} å­—ç¬¦")
            print("HTMLå‰500å­—ç¬¦:")
            print(html[:500])
            print("=" * 50)
        
        # ä¸»è¦æœç´¢ç»“æœé“¾æ¥ - è¿™æ˜¯æœ€å¯é çš„æ–¹å¼
        h2_pattern = r'<h2[^>]*><a[^>]*href=["\']([^"\']+)["\']'
        h2_matches = re.findall(h2_pattern, html, re.IGNORECASE)
        
        exclude_domains = ['bing.com', 'microsoft.com', 'msn.com', 'live.com', 'microsofttranslator.com']
        
        if getattr(st.session_state, 'debug_mode', False):
            print(f"ä»h2æ ‡ç­¾æ‰¾åˆ° {len(h2_matches)} ä¸ªé“¾æ¥")
        
        for href in h2_matches:
            try:
                # æ¸…ç†URL
                if '&amp;' in href:
                    href = href.split('&amp;')[0]
                
                if href.startswith(('http://', 'https://')):
                    if not any(domain in href for domain in exclude_domains):
                        urls.append(href)
                        if getattr(st.session_state, 'debug_mode', False):
                            print(f"âœ… æ·»åŠ URL: {href}")
                        
                        if len(urls) >= max_results:
                            break
                            
            except Exception as e:
                if getattr(st.session_state, 'debug_mode', False):
                    print(f"âŒ å¤„ç†URLæ—¶å‡ºé”™: {e}")
                continue
        
        if getattr(st.session_state, 'debug_mode', False):
            print(f"æœ€ç»ˆæå–åˆ° {len(urls)} ä¸ªæœ‰æ•ˆURL")
            for i, url in enumerate(urls, 1):
                print(f"{i}. {url}")
        
        return urls
    
    def check_robots_txt(self, urls: list[str]) -> list[str]:
        """æ£€æŸ¥robots.txt"""
        allowed_urls = []
        for url in urls:
            try:
                robots_url = f"{urlparse(url).scheme}://{urlparse(url).netloc}/robots.txt"
                rp = RobotFileParser(robots_url)
                rp.read()
                if rp.can_fetch("*", url):
                    allowed_urls.append(url)
            except Exception:
                allowed_urls.append(url)
        return allowed_urls
    
    def crawl_webpages(self, urls: list[str], query: str) -> list[CrawlResult]:
        """çˆ¬å–ç½‘é¡µå†…å®¹"""
        import asyncio
        
        try:
            return asyncio.run(self._crawl_webpages_async(urls, query))
        except Exception as e:
            st.error(f"ç½‘é¡µçˆ¬å–å¤±è´¥: {str(e)}")
            return []
    
    async def _crawl_webpages_async(self, urls: list[str], query: str) -> list[CrawlResult]:
        """å¼‚æ­¥çˆ¬å–ç½‘é¡µå†…å®¹"""
        try:
            bm25_filter = BM25ContentFilter(user_query=query, bm25_threshold=0.5)
            md_generator = DefaultMarkdownGenerator(content_filter=bm25_filter)

            crawler_config = CrawlerRunConfig(
                markdown_generator=md_generator,
                excluded_tags=["nav", "footer", "header", "form", "img", "a"],
                only_text=True,
                exclude_social_media_links=True,
                keep_data_attributes=False,
                cache_mode=CacheMode.BYPASS,
                remove_overlay_elements=True,
                user_agent="Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36",
                page_timeout=20000,
            )
            
            browser_config = BrowserConfig(headless=True, text_mode=True, light_mode=True)

            async with AsyncWebCrawler(config=browser_config) as crawler:
                results = await crawler.arun_many(urls, config=crawler_config)
                return results
        except Exception as e:
            st.error(f"å¼‚æ­¥ç½‘é¡µçˆ¬å–å¤±è´¥: {str(e)}")
            return []
    
    def add_to_vector_database(self, results: list[CrawlResult]):
        """å°†çˆ¬å–ç»“æœæ·»åŠ åˆ°å‘é‡æ•°æ®åº“"""
        total_documents = 0
        
        for result in results:
            documents, metadatas, ids = [], [], []
            
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=500,
                chunk_overlap=150,
                separators=["\n\n", "\n", "ã€‚", "ï¼Ÿ", "ï¼", ". ", ", ", " ", ""]
            )
            
            if result.markdown_v2 and result.markdown_v2.fit_markdown:
                markdown_result = result.markdown_v2.fit_markdown
            else:
                continue

            temp_file = tempfile.NamedTemporaryFile("w", suffix=".md", delete=False, encoding='utf-8')
            temp_file.write(markdown_result)
            temp_file.flush()

            try:
                loader = UnstructuredMarkdownLoader(temp_file.name, mode="single")
                docs = loader.load()
                all_splits = text_splitter.split_documents(docs)
            except Exception:
                all_splits = []
            finally:
                os.unlink(temp_file.name)

            normalized_url = self.normalize_url(result.url)

            if all_splits:
                for idx, split in enumerate(all_splits):
                    documents.append(split.page_content)
                    metadatas.append({"source": result.url})
                    ids.append(f"{normalized_url}_{idx}")

                self.collection.upsert(
                    documents=documents,
                    metadatas=metadatas,
                    ids=ids,
                )
                total_documents += len(documents)
        
        return total_documents
    
    def normalize_url(self, url):
        """è§„èŒƒåŒ–URL"""
        return (
            url.replace("https://", "")
            .replace("www.", "")
            .replace("/", "_")
            .replace("-", "_")
            .replace(".", "_")
        )
    
    def process_pdf_with_ocr(self, pdf_file, api_key: str) -> str:
        """ä½¿ç”¨OCRå¤„ç†PDFæ–‡ä»¶ï¼Œæ”¯æŒç¼“å­˜åŠŸèƒ½"""
        # è¯»å–æ–‡ä»¶å†…å®¹å¹¶è®¡ç®—å“ˆå¸Œ
        file_content = pdf_file.read()
        file_hash = self.get_file_hash(file_content)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ç¼“å­˜çš„ç»“æœ
        cached_result = self.get_cached_ocr_result(file_hash)
        if cached_result:
            st.info("âœ… ä½¿ç”¨ç¼“å­˜çš„OCRç»“æœï¼Œè·³è¿‡é‡å¤å¤„ç†")
            return cached_result
        
        # ä½¿ç”¨æä¾›çš„API Keyåˆå§‹åŒ–OCRå¤„ç†å™¨
        self.init_ocr_processor(api_key)
        
        if not self.ocr_processor:
            raise Exception("OCRå¤„ç†å™¨æœªåˆå§‹åŒ–ï¼Œè¯·æ£€æŸ¥MISTRAL_API_KEY")
        
        # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶åˆ°ä¸´æ—¶ä½ç½®
        with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
            tmp_file.write(file_content)
            tmp_file_path = tmp_file.name
        
        try:
            # åˆ›å»ºä¸´æ—¶è¾“å‡ºç›®å½•
            output_dir = tempfile.mkdtemp()
            
            # è¿›è¡ŒOCRå¤„ç†
            response_data, extracted_images = self.ocr_processor.process_pdf(
                tmp_file_path, output_dir, "markdown"
            )
            
            # è¯»å–ç”Ÿæˆçš„markdownæ–‡ä»¶
            markdown_file = os.path.join(output_dir, "ocr_result.md")
            if os.path.exists(markdown_file):
                with open(markdown_file, 'r', encoding='utf-8') as f:
                    markdown_content = f.read()
                
                # ä¿å­˜åˆ°ç¼“å­˜
                self.save_ocr_result_to_cache(file_hash, markdown_content)
                
                return markdown_content
            else:
                raise Exception("OCRå¤„ç†å¤±è´¥ï¼Œæœªç”Ÿæˆmarkdownæ–‡ä»¶")
                
        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            try:
                os.unlink(tmp_file_path)
            except Exception:
                pass
    
    def parse_report_sections(self, markdown_content: str) -> list[dict]:
        """è§£ææŠ¥å‘Šçš„ç« èŠ‚ç»“æ„"""
        sections = []
        lines = markdown_content.split('\n')
        current_section = {"title": "", "content": "", "level": 0}
        
        for line in lines:
            if line.strip().startswith('#'):
                # ä¿å­˜å‰ä¸€ä¸ªsection
                if current_section["title"]:
                    sections.append(current_section.copy())
                
                # å¼€å§‹æ–°çš„section
                level = len(line) - len(line.lstrip('#'))
                title = line.strip('#').strip()
                current_section = {
                    "title": title,
                    "content": line + "\n",
                    "level": level
                }
            else:
                current_section["content"] += line + "\n"
        
        # æ·»åŠ æœ€åä¸€ä¸ªsection
        if current_section["title"]:
            sections.append(current_section)
        
        return sections
    
    def query_vector_db(self, query: str, n_results: int = 10) -> str:
        """æŸ¥è¯¢å‘é‡æ•°æ®åº“"""
        try:
            results = self.collection.query(query_texts=[query], n_results=n_results)
            documents = results.get("documents")[0] if results.get("documents") else []
            return " ".join(documents)
        except Exception:
            return ""
    
    def filter_think_content(self, content: str) -> str:
        """è¿‡æ»¤æ‰å†…å®¹ä¸­çš„<think></think>æ ‡ç­¾åŠå…¶å†…å®¹"""
        if not content:
            return content
            
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ç§»é™¤<think>...</think>å†…å®¹ï¼Œæ”¯æŒå¤šè¡Œå’ŒåµŒå¥—
        patterns = [
            r'<think>.*?</think>',  # æ ‡å‡†æ ¼å¼
            r'<thinking>.*?</thinking>',  # å¯èƒ½çš„å˜ä½“
            r'<THINK>.*?</THINK>',  # å¤§å†™å˜ä½“
            r'<THINKING>.*?</THINKING>',  # å¤§å†™å˜ä½“
        ]
        
        cleaned_content = content
        for pattern in patterns:
            cleaned_content = re.sub(pattern, '', cleaned_content, flags=re.DOTALL | re.IGNORECASE)
        
        # æ¸…ç†å¤šä½™çš„ç©ºè¡Œå’Œç©ºç™½å­—ç¬¦
        cleaned_content = re.sub(r'\n\s*\n\s*\n+', '\n\n', cleaned_content)
        cleaned_content = re.sub(r'^\s+|\s+$', '', cleaned_content, flags=re.MULTILINE)
        
        return cleaned_content.strip()
    
    def call_llm(self, prompt: str, context: str = "") -> str:
        """è°ƒç”¨LLMç”Ÿæˆå†…å®¹"""
        try:
            full_prompt = prompt.format(context=context)
            
            response = ollama.chat(
                model="deepseek-r1:32b",
                messages=[{
                    "role": "user",
                    "content": full_prompt
                }]
            )
            
            # è¿‡æ»¤æ‰æ€è€ƒå†…å®¹
            raw_content = response["message"]["content"]
            filtered_content = self.filter_think_content(raw_content)
            
            return filtered_content
        except Exception as e:
            return f"LLMè°ƒç”¨å¤±è´¥: {str(e)}"
    
    def generate_financial_summary(self, applicant_name: str) -> str:
        """ç”Ÿæˆè´¢æŠ¥æ€»ç»“"""
        context = self.query_vector_db(f"{applicant_name} è´¢åŠ¡ ç»è¥")
        return self.call_llm(FINANCIAL_SUMMARY_PROMPT, context)
    
    def fill_report_section(self, section_content: str, applicant_name: str) -> str:
        """å¡«å†™æŠ¥å‘Šç« èŠ‚"""
        # æå–ç« èŠ‚ä¸­çš„å…³é”®ä¿¡æ¯è¿›è¡Œå‘é‡æ£€ç´¢
        section_keywords = self.extract_section_keywords(section_content)
        # ç»“åˆç”³è¯·äººåç§°å’Œç« èŠ‚å…³é”®è¯è¿›è¡Œæ£€ç´¢
        query = f"{applicant_name} {section_keywords}"
        context = self.query_vector_db(query)
        return self.call_llm(REPORT_FILLING_PROMPT.format(section_content=section_content, context=context))
    
    def extract_section_keywords(self, section_content: str) -> str:
        """ä»ç« èŠ‚å†…å®¹ä¸­æå–å…³é”®è¯ç”¨äºå‘é‡æ£€ç´¢"""
        # ç§»é™¤markdownè¯­æ³•æ ‡è®°
        import re
        text = re.sub(r'[#*\-|]', ' ', section_content)
        
        # æå–å…³é”®è¯ï¼ˆä¸­æ–‡è¯æ±‡ï¼‰
        keywords = re.findall(r'[\u4e00-\u9fff]+', text)
        
        # è¿‡æ»¤æ‰è¿‡çŸ­çš„è¯å’Œå¸¸è§è¯
        filtered_keywords = []
        common_words = {'çš„', 'æ˜¯', 'åœ¨', 'æœ‰', 'å’Œ', 'ä¸', 'æˆ–', 'ä½†', 'è€Œ', 'äº†', 'ä¹Ÿ', 'å°±', 'éƒ½', 'å°†', 'ä¸º', 'ä»', 'ç­‰', 'åŠ', 'ä»¥', 'å¯', 'èƒ½', 'åº”', 'è¦', 'å¦‚', 'ä¸‹', 'ä¸Š', 'ä¸­', 'å†…', 'å¤–', 'å‰', 'å', 'å·¦', 'å³'}
        
        for keyword in keywords:
            if len(keyword) >= 2 and keyword not in common_words:
                filtered_keywords.append(keyword)
        
        # è¿”å›å‰10ä¸ªå…³é”®è¯
        return ' '.join(filtered_keywords[:10])
    
    def convert_markdown_to_word(self, markdown_text: str, filename: str = "output.docx") -> bytes:
        """å°†Markdownè½¬æ¢ä¸ºWordæ–‡æ¡£"""
        try:
            # åˆ›å»ºä¸´æ—¶ç›®å½•
            temp_dir = tempfile.mkdtemp()
            
            # ä¿å­˜Markdownåˆ°ä¸´æ—¶æ–‡ä»¶
            qmd_path = os.path.join(temp_dir, "output.qmd")
            with open(qmd_path, "w", encoding="utf-8") as f:
                f.write(markdown_text)
            
            # ä½¿ç”¨quartoè½¬æ¢
            docx_filename = filename
            docx_path = os.path.join(temp_dir, docx_filename)
            
            current_dir = os.getcwd()
            try:
                os.chdir(temp_dir)
                subprocess.run(
                    ["quarto", "render", "output.qmd", "--to", "docx", "-o", docx_filename],
                    check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE
                )
            finally:
                os.chdir(current_dir)
            
            # è¯»å–ç”Ÿæˆçš„æ–‡æ¡£
            with open(docx_path, "rb") as f:
                docx_bytes = f.read()
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            shutil.rmtree(temp_dir, ignore_errors=True)
            
            return docx_bytes
            
        except Exception as e:
            raise Exception(f"è½¬æ¢å¤±è´¥: {str(e)}")
    
    def clear_ocr_cache(self):
        """æ¸…ç†OCRç¼“å­˜"""
        try:
            if self.ocr_cache_dir.exists():
                shutil.rmtree(self.ocr_cache_dir)
                self.ocr_cache_dir.mkdir(exist_ok=True)
                return True
        except Exception:
            pass
        return False
    
    def get_cache_info(self):
        """è·å–ç¼“å­˜ä¿¡æ¯"""
        if not self.ocr_cache_dir.exists():
            return {"count": 0, "size": "0 MB"}
        
        try:
            cache_files = list(self.ocr_cache_dir.glob("*.md"))
            count = len(cache_files)
            
            total_size = sum(f.stat().st_size for f in cache_files)
            size_mb = total_size / (1024 * 1024)
            
            return {
                "count": count,
                "size": f"{size_mb:.2f} MB"
            }
        except Exception:
            return {"count": 0, "size": "0 MB"}

# ä¸»åº”ç”¨
def main():
    st.set_page_config(
        page_title=get_text("page_title"),
        page_icon="ğŸ¦",
        layout="wide"
    )
    
    st.header(get_text("header"))
    
    # åˆå§‹åŒ–å¤„ç†å™¨
    if 'processor' not in st.session_state:
        st.session_state.processor = CreditReportProcessor()
    
    processor = st.session_state.processor
    
    # ä¾§è¾¹æ è¾“å…¥
    with st.sidebar:
        st.subheader("ğŸ“ è¾“å…¥ä¿¡æ¯")
        
        # ç”³è¯·äººåç§°è¾“å…¥
        applicant_name = st.text_input(
            get_text("applicant_name"),
            value="é”é“ä¼ä¸šç®¡ç†å’¨è¯¢(ä¸Šæµ·)æœ‰é™å…¬å¸",
            placeholder=get_text("applicant_placeholder"),
            key="applicant_name"
        )
        
        # OCR API Keyè¾“å…¥
        ocr_api_key = st.text_input(
            "Mistral OCR API Key",
            value="EoBu0h6XasHmXH2Y2izoqWO43shRUT4D",
            placeholder="è¯·è¾“å…¥Mistral OCR API Key",
            type="password",
            key="ocr_api_key",
            help="ç”¨äºPDF OCRå¤„ç†çš„Mistral APIå¯†é’¥"
        )
        
        # PDFæ–‡ä»¶ä¸Šä¼ 
        uploaded_file = st.file_uploader(
            get_text("upload_report"),
            type=["pdf"],
            key="pdf_upload"
        )
        
        # å¤„ç†æŒ‰é’®
        process_clicked = st.button(
            get_text("process_button"),
            type="primary",
            disabled=not (applicant_name and uploaded_file and ocr_api_key)
        )
        
        # OCRç¼“å­˜ç®¡ç†
        st.subheader("ğŸ—‚ï¸ OCRç¼“å­˜ç®¡ç†")
        cache_info = processor.get_cache_info()
        st.info(f"ç¼“å­˜æ–‡ä»¶æ•°é‡: {cache_info['count']}\nç¼“å­˜å¤§å°: {cache_info['size']}")
        
        if st.button("ğŸ—‘ï¸ æ¸…ç†OCRç¼“å­˜"):
            if processor.clear_ocr_cache():
                st.success("âœ… OCRç¼“å­˜å·²æ¸…ç†")
                st.rerun()
            else:
                st.error("âŒ æ¸…ç†ç¼“å­˜å¤±è´¥")
        
        # è°ƒè¯•æ¨¡å¼å¼€å…³
        st.subheader("ğŸ”§ è°ƒè¯•é€‰é¡¹")
        debug_mode = st.checkbox("æ˜¾ç¤ºè¯¦ç»†æœç´¢æ—¥å¿—", value=False, help="å¼€å¯åä¼šæ˜¾ç¤ºè¯¦ç»†çš„æœç´¢è¿‡ç¨‹ä¿¡æ¯")
        
        # å°†è°ƒè¯•æ¨¡å¼çŠ¶æ€ä¿å­˜åˆ°session state
        st.session_state.debug_mode = debug_mode
    
    # ä¸»å†…å®¹åŒºåŸŸ
    col1, col2 = st.columns([1, 1])
    
    with col1:
        st.subheader("ğŸ“Š è´¢æŠ¥æ€»ç»“")
        summary_placeholder = st.empty()
        
        if 'financial_summary' in st.session_state:
            summary_placeholder.markdown(st.session_state.financial_summary)
            
            # ä¸‹è½½è´¢æŠ¥æ€»ç»“æŒ‰é’®
            if st.button(get_text("download_summary")):
                try:
                    docx_bytes = processor.convert_markdown_to_word(
                        st.session_state.financial_summary,
                        f"{applicant_name}_è´¢æŠ¥æ€»ç»“.docx"
                    )
                    st.download_button(
                        label="ğŸ’¾ ä¸‹è½½è´¢æŠ¥æ€»ç»“æ–‡æ¡£",
                        data=docx_bytes,
                        file_name=f"{applicant_name}_è´¢æŠ¥æ€»ç»“.docx",
                        mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document"
                    )
                except Exception as e:
                    st.error(f"ç”Ÿæˆæ–‡æ¡£å¤±è´¥: {str(e)}")
    
    with col2:
        st.subheader("ğŸ“‹ å¡«å†™è¿›åº¦")
        progress_placeholder = st.empty()
        filled_content_placeholder = st.empty()
        
        if 'filled_report' in st.session_state:
            filled_content_placeholder.markdown(st.session_state.filled_report)
            
            # ä¸‹è½½å¡«å†™å®Œæˆçš„æŠ¥å‘ŠæŒ‰é’®
            if st.button(get_text("download_filled")):
                try:
                    docx_bytes = processor.convert_markdown_to_word(
                        st.session_state.filled_report,
                        f"{applicant_name}_æˆä¿¡è°ƒæŸ¥æŠ¥å‘Š.docx"
                    )
                    st.download_button(
                        label="ğŸ’¾ ä¸‹è½½å¡«å†™å®Œæˆçš„æŠ¥å‘Š",
                        data=docx_bytes,
                        file_name=f"{applicant_name}_æˆä¿¡è°ƒæŸ¥æŠ¥å‘Š.docx",
                        mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document"
                    )
                except Exception as e:
                    st.error(f"ç”Ÿæˆæ–‡æ¡£å¤±è´¥: {str(e)}")
    
    # å¤„ç†é€»è¾‘
    if process_clicked:
        try:
            # 1. æœç´¢ç”³è¯·äººä¿¡æ¯
            with st.spinner(get_text("searching")):
                urls = processor.search_applicant_info(applicant_name)
                if urls:
                    st.success(get_text("found_urls").format(len(urls)))
                    
                    # çˆ¬å–ç½‘é¡µå¹¶å»ºç«‹å‘é‡æ•°æ®åº“
                    results = processor.crawl_webpages(urls, applicant_name)
                    total_docs = processor.add_to_vector_database(results)
                    st.info(f"ğŸ“š å·²å°† {total_docs} ä¸ªæ–‡æ¡£ç‰‡æ®µæ·»åŠ åˆ°æ•°æ®åº“")
                else:
                    st.warning("æœªæ‰¾åˆ°ç›¸å…³ç½‘é¡µä¿¡æ¯")
            
            # 2. OCRå¤„ç†PDF
            with st.spinner(get_text("ocr_processing")):
                markdown_content = processor.process_pdf_with_ocr(uploaded_file, ocr_api_key)
                st.success("âœ… OCRå¤„ç†å®Œæˆ")
            
            # 3. ç”Ÿæˆè´¢æŠ¥æ€»ç»“
            with st.spinner(get_text("generating_summary")):
                financial_summary = processor.generate_financial_summary(applicant_name)
                # ç¡®ä¿è¿‡æ»¤æ‰thinkå†…å®¹
                financial_summary = processor.filter_think_content(financial_summary)
                st.session_state.financial_summary = financial_summary
                summary_placeholder.markdown(financial_summary)
            
            # 4. è§£ææŠ¥å‘Šç»“æ„å¹¶é€æ®µå¡«å†™
            with st.spinner(get_text("report_parsing")):
                sections = processor.parse_report_sections(markdown_content)
                st.info(f"ğŸ“‹ è§£æåˆ° {len(sections)} ä¸ªæŠ¥å‘Šæ®µè½")
            
            # 5. é€æ®µå¡«å†™æŠ¥å‘Š
            filled_sections = []
            progress_bar = st.progress(0)
            
            for i, section in enumerate(sections):
                section_title = section["title"] or f"ç¬¬{i+1}æ®µ"
                
                # æ›´æ–°è¿›åº¦
                progress = (i + 1) / len(sections)
                progress_bar.progress(progress)
                progress_placeholder.text(get_text("filling_report").format(i+1, section_title))
                
                # å¡«å†™å½“å‰æ®µè½
                filled_content = processor.fill_report_section(section["content"], applicant_name)
                # ç¡®ä¿è¿‡æ»¤æ‰thinkå†…å®¹
                filled_content = processor.filter_think_content(filled_content)
                filled_sections.append(filled_content)
                
                # å®æ—¶æ›´æ–°æ˜¾ç¤ºå†…å®¹
                current_filled = "\n\n".join(filled_sections)
                st.session_state.filled_report = current_filled
                filled_content_placeholder.markdown(current_filled)
            
            # å®Œæˆå¤„ç†
            progress_placeholder.success(get_text("processing_complete"))
            st.balloons()
            
        except Exception as e:
            st.error(f"{get_text('error_occurred')}: {str(e)}")

if __name__ == "__main__":
    main() 