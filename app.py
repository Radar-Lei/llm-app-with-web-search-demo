"""
LLM App with Web Search
"""

import asyncio
import os
import tempfile
import re
from urllib.parse import urlparse, quote_plus
from urllib.robotparser import RobotFileParser

import chromadb
import ollama
import streamlit as st
from chromadb.config import Settings
from chromadb.utils.embedding_functions import OllamaEmbeddingFunction
from crawl4ai import AsyncWebCrawler, BrowserConfig, CacheMode, CrawlerRunConfig
from crawl4ai.content_filter_strategy import BM25ContentFilter
from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator
from crawl4ai.models import CrawlResult
from langchain_community.document_loaders import UnstructuredMarkdownLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter

system_prompt = """
You are an AI assistant tasked with providing detailed answers based solely on the given context.
Your goal is to analyze the information provided and formulate a comprehensive, well-structured response to the question.

Context will be passed as "Context:"
User question will be passed as "Question:"

To answer the question:
1. Thoroughly analyze the context, identifying key information relevant to the question.
2. Organize your thoughts and plan your response to ensure a logical flow of information.
3. Formulate a detailed answer that directly addresses the question, using only the information provided in the context.
4. When the context supports an answer, ensure your response is clear, concise, and directly addresses the question.
5. When there is no context, just say you have no context and stop immediately.
6. If the context doesn't contain sufficient information to fully answer the question, state this clearly in your response.
7. Avoid explaining why you cannot answer or speculating about missing details. Simply state that you lack sufficient context when necessary.

Format your response as follows:
1. Use clear, concise language.
2. Organize your answer into paragraphs for readability.
3. Use bullet points or numbered lists where appropriate to break down complex information.
4. If relevant, include any headings or subheadings to structure your response.
5. Ensure proper grammar, punctuation, and spelling throughout your answer.
6. Do not mention what you received in context, just focus on answering based on the context.

Important: Base your entire response solely on the information provided in the context. Do not include any external knowledge or assumptions not present in the given text.
"""


def call_llm(prompt: str, with_context: bool = True, context: str | None = None):
    """Calls the LLM model with the given prompt and optional context.

    Args:
        prompt (str): The user prompt/question to send to the LLM
        with_context (bool, optional): Whether to include system context. Defaults to True.
        context (str | None, optional): Additional context to provide to the LLM. Defaults to None.

    Yields:
        str: Generated text chunks from the LLM response stream

    Returns:
        Generator[str, None, None]: A generator yielding response chunks
    """
    messages = [
        {
            "role": "system",
            "content": system_prompt,
        },
        {
            "role": "user",
            "content": f"Context: {context}, Question: {prompt}",
        },
    ]

    if not with_context:
        messages.pop(0)
        messages[0]["content"] = prompt

    response = ollama.chat(model="deepseek-r1:32b", stream=True, messages=messages)

    for chunk in response:
        if chunk["done"] is False:
            yield chunk["message"]["content"]
        else:
            break


def get_vector_collection() -> tuple[chromadb.Collection, chromadb.Client]:
    """Creates or retrieves a vector collection for storing embeddings.

    Creates an embedding function using Ollama and initializes a persistent ChromaDB client.
    Returns both the collection and client objects.

    Returns:
        tuple[chromadb.Collection, chromadb.Client]: A tuple containing:
            - The ChromaDB collection for storing embeddings
            - The ChromaDB client instance
    """
    ollama_ef = OllamaEmbeddingFunction(
        url="http://localhost:11434/api/embeddings",
        model_name="paraphrase-multilingual:latest",
    )

    chroma_client = chromadb.PersistentClient(
        path="./web-search-llm-db", settings=Settings(anonymized_telemetry=False)
    )
    return (
        chroma_client.get_or_create_collection(
            name="web_llm",
            embedding_function=ollama_ef,
            metadata={"hnsw:space": "cosine"},
        ),
        chroma_client,
    )


def normalize_url(url):
    """Normalizes a URL by removing common prefixes and replacing special characters.

    Args:
        url (str): The URL to normalize.

    Returns:
        str: The normalized URL with https://, www. removed and /, -, . replaced with underscores.

    Example:
        >>> normalize_url("https://www.example.com/path")
        "example_com_path"
    """
    normalized_url = (
        url.replace("https://", "")
        .replace("www.", "")
        .replace("/", "_")
        .replace("-", "_")
        .replace(".", "_")
    )
    print("Normalized URL", normalized_url)
    return normalized_url


def add_to_vector_database(results: list[CrawlResult]):
    """Adds crawl results to a vector database for semantic search.

    Takes a list of crawl results, processes the markdown content by splitting it into chunks,
    and stores the chunks in a ChromaDB vector collection with associated metadata.

    Args:
        results (list[CrawlResult]): List of crawl results containing markdown content and URLs

    Returns:
        None

    Note:
        - Uses RecursiveCharacterTextSplitter to split markdown into chunks
        - Creates temporary markdown files for processing
        - Normalizes URLs for use as document IDs
        - Upserts documents, metadata and IDs to ChromaDB collection
    """
    collection, _ = get_vector_collection()
    total_documents = 0

    for result in results:
        documents, metadatas, ids = [], [], []

        # ä¼˜åŒ–ä¸­æ–‡æ–‡æœ¬åˆ†å‰²å™¨é…ç½®
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=500,  # å¢åŠ å—å¤§å°
            chunk_overlap=150,  # å¢åŠ é‡å 
            separators=["\n\n", "\n", "ã€‚", "ï¼Ÿ", "ï¼", ". ", ", ", " ", ""]  # ä¼˜åŒ–åˆ†éš”ç¬¦é¡ºåº
        )
        
        if result.markdown_v2 and result.markdown_v2.fit_markdown:
            markdown_result = result.markdown_v2.fit_markdown
            print(f"å¤„ç† URL: {result.url}, åŸå§‹å†…å®¹é•¿åº¦: {len(markdown_result)}")
        else:
            print(f"è·³è¿‡ URL: {result.url}, æ— å†…å®¹")
            continue

        temp_file = tempfile.NamedTemporaryFile("w", suffix=".md", delete=False, encoding='utf-8')
        temp_file.write(markdown_result)
        temp_file.flush()

        try:
            loader = UnstructuredMarkdownLoader(temp_file.name, mode="single")
            docs = loader.load()
            all_splits = text_splitter.split_documents(docs)
            print(f"æ–‡æœ¬åˆ†å‰²ç»“æœ: {len(all_splits)} ä¸ªå—")
        except Exception as e:
            print(f"å¤„ç†æ–‡æ¡£æ—¶å‡ºé”™: {e}")
            all_splits = []
        finally:
            os.unlink(temp_file.name)  # Delete the temporary file

        normalized_url = normalize_url(result.url)

        if all_splits:
            for idx, split in enumerate(all_splits):
                documents.append(split.page_content)
                metadatas.append({"source": result.url})
                ids.append(f"{normalized_url}_{idx}")

            print(f"å‘é‡æ•°æ®åº“æ’å…¥: {len(documents)} ä¸ªæ–‡æ¡£å—")
            collection.upsert(
                documents=documents,
                metadatas=metadatas,
                ids=ids,
            )
            total_documents += len(documents)
        else:
            print(f"è­¦å‘Š: URL {result.url} æ²¡æœ‰ç”Ÿæˆä»»ä½•æ–‡æ¡£å—")
    
    print(f"æ€»å…±æ’å…¥äº† {total_documents} ä¸ªæ–‡æ¡£å—åˆ°å‘é‡æ•°æ®åº“")


async def crawl_webpages(urls: list[str], prompt: str) -> CrawlResult:
    """Asynchronously crawls multiple webpages and extracts relevant content based on a prompt.

    Args:
        urls (list[str]): List of URLs to crawl
        prompt (str): Query text used to filter relevant content from the pages

    Returns:
        CrawlResult: Results from crawling containing filtered markdown content and metadata

    Note:
        Uses BM25 content filtering to extract relevant sections based on the prompt.
        Configures crawler to exclude navigation elements, forms, images etc.
        Runs in headless browser mode with text-only extraction.
    """
    import re as regex_module  # é¿å…å‘½åå†²çª
    
    # ä¸­æ–‡æŸ¥è¯¢ä½¿ç”¨æ›´å®½æ¾çš„è®¾ç½®ï¼Œé¿å…è¿‡åº¦è¿‡æ»¤
    is_chinese = regex_module.search(r'[\u4e00-\u9fff]', prompt)
    
    if is_chinese:
        # å¯¹ä¸­æ–‡æŸ¥è¯¢ï¼Œä½¿ç”¨å¾ˆä½çš„é˜ˆå€¼æˆ–ä¸ä½¿ç”¨BM25è¿‡æ»¤å™¨
        bm25_filter = BM25ContentFilter(user_query=prompt, bm25_threshold=0.1)
        print(f"ä½¿ç”¨ä¸­æ–‡BM25è¿‡æ»¤å™¨ï¼Œé˜ˆå€¼: 0.1, æŸ¥è¯¢: {prompt}")
    else:
        bm25_filter = BM25ContentFilter(user_query=prompt, bm25_threshold=1.2)
        print(f"ä½¿ç”¨è‹±æ–‡BM25è¿‡æ»¤å™¨ï¼Œé˜ˆå€¼: 1.2, æŸ¥è¯¢: {prompt}")

    # å¯¹äºä¸­æ–‡æŸ¥è¯¢ï¼Œæˆ‘ä»¬å¯èƒ½éœ€è¦è€ƒè™‘ä½¿ç”¨ä¸åŒçš„markdownç”Ÿæˆç­–ç•¥
    if is_chinese:
        # ä¸ºä¸­æ–‡ä½¿ç”¨æ›´å®½æ¾çš„markdownç”Ÿæˆç­–ç•¥
        md_generator = DefaultMarkdownGenerator(
            content_filter=bm25_filter,
            # ä¸è¦å¤ªä¸¥æ ¼åœ°è¿‡æ»¤å†…å®¹
        )
    else:
        md_generator = DefaultMarkdownGenerator(content_filter=bm25_filter)

    crawler_config = CrawlerRunConfig(
        markdown_generator=md_generator,
        excluded_tags=["nav", "footer", "header", "form", "img", "a"],
        only_text=True,
        exclude_social_media_links=True,
        keep_data_attributes=False,
        cache_mode=CacheMode.BYPASS,
        remove_overlay_elements=True,
        user_agent="Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/132.0.0.0 Safari/537.36",
        page_timeout=20000,  # in ms: 20 seconds
    )
    browser_config = BrowserConfig(headless=True, text_mode=True, light_mode=True)

    async with AsyncWebCrawler(config=browser_config) as crawler:
        results = await crawler.arun_many(urls, config=crawler_config)
        
        # æ·»åŠ è°ƒè¯•ä¿¡æ¯
        for i, result in enumerate(results):
            if result.markdown_v2 and result.markdown_v2.fit_markdown:
                content_length = len(result.markdown_v2.fit_markdown)
                print(f"çˆ¬å–ç»“æœ {i+1}: URL={result.url[:50]}..., å†…å®¹é•¿åº¦={content_length}")
                
                # å¦‚æœä¸­æ–‡æŸ¥è¯¢çš„å†…å®¹å¤ªå°‘ï¼Œå°è¯•ä½¿ç”¨åŸå§‹å†…å®¹
                if is_chinese and content_length < 50:
                    print(f"ä¸­æ–‡å†…å®¹å¤ªå°‘ï¼Œå°è¯•ä½¿ç”¨åŸå§‹markdown...")
                    if hasattr(result, 'markdown') and result.markdown:
                        result.markdown_v2.fit_markdown = result.markdown
                        print(f"ä½¿ç”¨åŸå§‹markdownï¼Œé•¿åº¦: {len(result.markdown)}")
                    elif hasattr(result, 'cleaned_html') and result.cleaned_html:
                        # å¦‚æœæœ‰æ¸…ç†è¿‡çš„HTMLï¼Œè½¬æ¢ä¸ºç®€å•æ–‡æœ¬
                        text = regex_module.sub(r'<[^>]+>', '', result.cleaned_html)
                        result.markdown_v2.fit_markdown = text
                        print(f"ä½¿ç”¨æ¸…ç†è¿‡çš„HTMLè½¬æ–‡æœ¬ï¼Œé•¿åº¦: {len(text)}")
            else:
                print(f"çˆ¬å–ç»“æœ {i+1}: URL={result.url[:50]}..., å†…å®¹ä¸ºç©º")
        
        return results


def check_robots_txt(urls: list[str]) -> list[str]:
    """Checks robots.txt files to determine which URLs are allowed to be crawled.

    Args:
        urls (list[str]): List of URLs to check against their robots.txt files.

    Returns:
        list[str]: List of URLs that are allowed to be crawled according to robots.txt rules.
            If a robots.txt file is missing or there's an error, the URL is assumed to be allowed.
    """
    allowed_urls = []

    for url in urls:
        try:
            robots_url = f"{urlparse(url).scheme}://{urlparse(url).netloc}/robots.txt"
            rp = RobotFileParser(robots_url)
            rp.read()

            if rp.can_fetch("*", url):
                allowed_urls.append(url)

        except Exception:
            # If robots.txt is missing or there's any error, assume URL is allowed
            allowed_urls.append(url)

    return allowed_urls


async def get_web_urls(search_term: str, num_results: int = 20) -> list[str]:
    """ä½¿ç”¨ crawl4ai é€šè¿‡ Bing æœç´¢å¹¶è¿”å›è¿‡æ»¤åçš„ URLã€‚

    ä½¿ç”¨æ— å¤´æµè§ˆå™¨è®¿é—® Bing æœç´¢é¡µé¢ï¼Œè§£ææœç´¢ç»“æœå¹¶æå–é“¾æ¥ï¼Œ
    æ’é™¤æŸäº›åŸŸåå¹¶æ£€æŸ¥ robots.txt åˆè§„æ€§ã€‚

    Args:
        search_term (str): è¦ä½¿ç”¨çš„æœç´¢æŸ¥è¯¢
        num_results (int, optional): è¿”å›çš„æœ€å¤§æœç´¢ç»“æœæ•°é‡ã€‚é»˜è®¤ä¸º 10ã€‚

    Returns:
        list[str]: æ ¹æ® robots.txt å…è®¸çˆ¬å–çš„ URL åˆ—è¡¨

    Raises:
        Exception: å¦‚æœç½‘ç»œæœç´¢å¤±è´¥ï¼Œæ‰“å°é”™è¯¯æ¶ˆæ¯å¹¶åœæ­¢æ‰§è¡Œ
    """
    try:
        # æ„å»º Bing æœç´¢ URL
        discard_domains = ["youtube.com", "britannica.com", "vimeo.com"]
        query = search_term
        # æ£€æŸ¥æ˜¯å¦åŒ…å«ä¸­æ–‡å­—ç¬¦
        if not re.search(r'[\u4e00-\u9fff]', query):
            for domain in discard_domains:
                query += f" -site:{domain}"
        
        encoded_query = quote_plus(query)
        search_url = f"https://www.bing.com/search?q={encoded_query}&count={num_results}"
        
        print(f"æ­£åœ¨æœç´¢: {search_url}")
        
        # é…ç½®çˆ¬è™«
        crawler_config = CrawlerRunConfig(
            excluded_tags=["nav", "footer", "header", "form", "img"],
            only_text=False,  # æˆ‘ä»¬éœ€è¦ä¿ç•™é“¾æ¥
            exclude_social_media_links=False,
            keep_data_attributes=False,
            cache_mode=CacheMode.BYPASS,
            remove_overlay_elements=True,
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
            page_timeout=30000,  # 30 ç§’è¶…æ—¶
            delay_before_return_html=3.0,  # ç­‰å¾…é¡µé¢åŠ è½½
        )
        
        browser_config = BrowserConfig(
            headless=True, 
            text_mode=False,  # éœ€è¦ HTML æ¥æå–é“¾æ¥
            light_mode=True
        )
        
        # çˆ¬å–æœç´¢ç»“æœé¡µé¢
        async with AsyncWebCrawler(config=browser_config) as crawler:
            result = await crawler.arun(url=search_url, config=crawler_config)
            
            if not result.html:
                raise Exception("æ— æ³•è·å–æœç´¢ç»“æœé¡µé¢çš„ HTML å†…å®¹")
            
            # è§£ææœç´¢ç»“æœé“¾æ¥
            urls = extract_bing_links(result.html, num_results)
            
            if not urls:
                print("æœªæ‰¾åˆ°æœç´¢ç»“æœé“¾æ¥")
                return []
            
            print(f"æ‰¾åˆ° {len(urls)} ä¸ªæœç´¢ç»“æœ")
            
            # æ£€æŸ¥ robots.txt åˆè§„æ€§
            return check_robots_txt(urls)
            
    except Exception as e:
        error_msg = f"âŒ ä»ç½‘ç»œè·å–ç»“æœå¤±è´¥: {str(e)}"
        print(error_msg)
        st.write(error_msg)
        st.stop()


def extract_bing_links(html: str, max_results: int = 10) -> list[str]:
    """ä» Bing æœç´¢ç»“æœ HTML ä¸­æå–é“¾æ¥ã€‚

    Args:
        html (str): Bing æœç´¢ç»“æœé¡µé¢çš„ HTML å†…å®¹
        max_results (int): æå–çš„æœ€å¤§é“¾æ¥æ•°é‡

    Returns:
        list[str]: æå–çš„ URL åˆ—è¡¨
    """
    from urllib.parse import unquote
    
    urls = []
    
    # æ¨¡å¼ 1: Bing æœç´¢ç»“æœé“¾æ¥ - æŸ¥æ‰¾æ ‡é¢˜é“¾æ¥
    # Bing é€šå¸¸åœ¨ h2 æ ‡ç­¾ä¸­åŒ…å«ç»“æœæ ‡é¢˜é“¾æ¥
    title_pattern = r'<h2[^>]*><a[^>]*href=["\']([^"\']+)["\']'
    title_matches = re.findall(title_pattern, html, re.IGNORECASE)
    
    for href in title_matches:
        try:
            if href.startswith(('http://', 'https://')):
                # æ’é™¤ Bing å’Œ Microsoft ç›¸å…³çš„åŸŸå
                exclude_domains = ['bing.com', 'microsoft.com', 'msn.com', 'live.com']
                if not any(domain in href for domain in exclude_domains):
                    urls.append(href)
                    if len(urls) >= max_results:
                        break
        except Exception:
            continue
    
    # æ¨¡å¼ 2: æŸ¥æ‰¾æ‰€æœ‰ä»¥ http å¼€å¤´çš„é“¾æ¥ï¼Œä½†æ’é™¤å¯¼èˆªå’Œå¹¿å‘Šé“¾æ¥
    general_pattern = r'href=["\']([^"\']*(?:https?://[^"\']+))["\']'
    general_matches = re.findall(general_pattern, html)
    
    exclude_domains = ['bing.com', 'microsoft.com', 'msn.com', 'live.com', 'microsofttranslator.com']
    exclude_keywords = ['privacy', 'terms', 'help', 'support', 'feedback', 'advertising']
    
    for href in general_matches:
        try:
            if href.startswith(('http://', 'https://')):
                # æ’é™¤ç‰¹å®šåŸŸåå’Œå…³é”®è¯
                if (not any(domain in href for domain in exclude_domains) and
                    not any(keyword in href.lower() for keyword in exclude_keywords)):
                    
                    # ç¡®ä¿é“¾æ¥çœ‹èµ·æ¥æ˜¯çœŸå®çš„å†…å®¹é¡µé¢
                    if len(href) > 20 and '.' in href:  # åŸºæœ¬çš„æœ‰æ•ˆæ€§æ£€æŸ¥
                        urls.append(href)
                        if len(urls) >= max_results:
                            break
        except Exception:
            continue
    
    # æ¨¡å¼ 3: æŸ¥æ‰¾ç‰¹å®šçš„ç»“æœé“¾æ¥ç±»
    result_link_pattern = r'<cite[^>]*>([^<]+)</cite>'
    cite_matches = re.findall(result_link_pattern, html, re.IGNORECASE)
    
    for cite_text in cite_matches:
        try:
            # ä» cite æ ‡ç­¾çš„æ–‡æœ¬ä¸­æå– URL
            if cite_text.startswith(('http://', 'https://')):
                exclude_domains = ['bing.com', 'microsoft.com', 'msn.com', 'live.com']
                if not any(domain in cite_text for domain in exclude_domains):
                    urls.append(cite_text.split(' ')[0])  # å–ç¬¬ä¸€ä¸ªå¯èƒ½çš„ URL
                    if len(urls) >= max_results:
                        break
            elif '/' in cite_text and '.' in cite_text:
                # å¯èƒ½æ˜¯çœç•¥äº†åè®®çš„ URL
                full_url = f"https://{cite_text.split(' ')[0]}"
                exclude_domains = ['bing.com', 'microsoft.com', 'msn.com', 'live.com']
                if not any(domain in full_url for domain in exclude_domains):
                    urls.append(full_url)
                    if len(urls) >= max_results:
                        break
        except Exception:
            continue
    
    # å»é‡
    unique_urls = []
    seen = set()
    for url in urls:
        if url not in seen:
            unique_urls.append(url)
            seen.add(url)
    
    print(f"æå–åˆ° {len(unique_urls)} ä¸ªå”¯ä¸€é“¾æ¥")
    return unique_urls[:max_results]


async def run():
    st.set_page_config(page_title="LLM with Web Search")

    st.header("ğŸ” å¤§æ¨¡å‹+ç½‘ç»œæœç´¢LLM Web Search")
    prompt = st.text_area(
        label="Put your query here",
        placeholder="Add your query...",
        label_visibility="hidden",
    )
    is_web_search = st.toggle("Enable web search", value=False, key="enable_web_search")
    go = st.button(
        "âš¡ï¸ Go",
    )

    collection, chroma_client = get_vector_collection()

    if prompt and go:
        if is_web_search:
            st.write(f"ğŸ” å¼€å§‹æœç´¢: {prompt}")
            
            web_urls = await get_web_urls(search_term=prompt)
            if not web_urls:
                st.write("âŒ æœªæ‰¾åˆ°æœç´¢ç»“æœã€‚")
                st.stop()

            st.write(f"ğŸ“ æ‰¾åˆ° {len(web_urls)} ä¸ªURLï¼Œå¼€å§‹çˆ¬å–...")
            results = await crawl_webpages(urls=web_urls, prompt=prompt)
            
            st.write("ğŸ’¾ å°†å†…å®¹æ·»åŠ åˆ°å‘é‡æ•°æ®åº“...")
            add_to_vector_database(results)

            st.write("ğŸ” ä»å‘é‡æ•°æ®åº“æŸ¥è¯¢ç›¸å…³å†…å®¹...")
            qresults = collection.query(query_texts=[prompt], n_results=10)
            context_docs = qresults.get("documents")[0] if qresults.get("documents") else []
            
            # æ·»åŠ è¯¦ç»†çš„è°ƒè¯•ä¿¡æ¯
            print(f"å‘é‡æŸ¥è¯¢ç»“æœ: {len(context_docs)} ä¸ªæ–‡æ¡£")
            if context_docs:
                total_context_length = sum(len(doc) for doc in context_docs)
                print(f"ä¸Šä¸‹æ–‡æ€»é•¿åº¦: {total_context_length} å­—ç¬¦")
                context = " ".join(context_docs)
                st.write(f"âœ… æ‰¾åˆ° {len(context_docs)} ä¸ªç›¸å…³æ–‡æ¡£ç‰‡æ®µï¼Œæ€»é•¿åº¦: {total_context_length} å­—ç¬¦")
            else:
                context = ""
                st.write("âš ï¸ è­¦å‘Š: æœªæ‰¾åˆ°ç›¸å…³çš„ä¸Šä¸‹æ–‡å†…å®¹")
                print("è­¦å‘Š: å‘é‡æŸ¥è¯¢è¿”å›ç©ºç»“æœ")

            chroma_client.delete_collection(
                name="web_llm"
            )  # Delete collection after use

            st.write("ğŸ¤– ç”Ÿæˆå›ç­”...")
            llm_response = call_llm(
                context=context, prompt=prompt, with_context=is_web_search
            )
            st.write_stream(llm_response)
        else:
            llm_response = call_llm(prompt=prompt, with_context=is_web_search)
            st.write_stream(llm_response)


if __name__ == "__main__":
    asyncio.run(run())
